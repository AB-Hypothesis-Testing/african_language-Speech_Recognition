{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speech_recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPoYMJo4ogzeBh6r6pzUn0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/African-language-Speech-Recognition/african_language-Speech_Recognition/blob/dan_preprocessing/notebooks/speech_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rccQ1MRc1np",
        "outputId": "687943b0-9dc9-4b4e-b7c2-17b1ef54b058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wave, array"
      ],
      "metadata": {
        "id": "7c8kpvZ-g4RY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd train/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Vr1krIep_1",
        "outputId": "2d982e3e-8a45-420a-e471-80f412958796"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Week-4-STT/data/AMHARIC/data/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python_speech_features"
      ],
      "metadata": {
        "id": "1Tjzhip5f4kS"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "23n4Jptqfscj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import soundfile\n",
        "import json\n",
        "\n",
        "import random\n",
        "from python_speech_features import mfcc\n",
        "import librosa\n",
        "import scipy.io.wavfile as wav\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
        "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "\n",
        "import pickle as pickle\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "\n",
        "from keras.layers import (Input, Lambda)\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint   \n",
        "import os"
      ],
      "metadata": {
        "id": "FpXP6V7bfb6-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kF1oqQAtliTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "id": "3qOEI1EMh20s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supported = \"\"\"\n",
        "ሀ ሁ ሂ ሄ ህ ሆ\n",
        "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
        "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
        "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
        "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
        "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
        "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
        "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
        "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
        "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
        "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
        "ኋ\n",
        "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
        "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
        "አ ኡ ኢ ኤ እ ኦ\n",
        "ኧ\n",
        "ከ ኩ ኪ ካ ኬ ክ ኮ\n",
        "ኳ\n",
        "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
        "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
        "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
        "የ ዩ ዪ ያ ዬ ይ ዮ\n",
        "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
        "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
        "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
        "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
        "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
        "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
        "ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
        "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
        "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
        "\"\"\".split()\n",
        "\n",
        "char_map = {}\n",
        "char_map[\"\"] = 0\n",
        "char_map[\"<SPACE>\"] = 1\n",
        "index = 2\n",
        "for c in supported:\n",
        "    char_map[c] = index\n",
        "    index += 1\n",
        "index_map = {v+1: k for k, v in char_map.items()}"
      ],
      "metadata": {
        "id": "EwpFkcNxhU7a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_feat_dim(window, max_freq):\n",
        "    return int(0.001 * window * max_freq) + 1"
      ],
      "metadata": {
        "id": "EqGJ2SBXhv0G"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
        "    \"\"\"\n",
        "    Compute the spectrogram for a real signal.\n",
        "    The parameters follow the naming convention of\n",
        "    matplotlib.mlab.specgram\n",
        "\n",
        "    Args:\n",
        "        samples (1D array): input audio signal\n",
        "        fft_length (int): number of elements in fft window\n",
        "        sample_rate (scalar): sample rate\n",
        "        hop_length (int): hop length (relative offset between neighboring\n",
        "            fft windows).\n",
        "\n",
        "    Returns:\n",
        "        x (2D array): spectrogram [frequency x time]\n",
        "        freq (1D array): frequency of each row in x\n",
        "\n",
        "    Note:\n",
        "        This is a truncating computation e.g. if fft_length=10,\n",
        "        hop_length=5 and the signal has 23 elements, then the\n",
        "        last 3 elements will be truncated.\n",
        "    \"\"\"\n",
        "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
        "\n",
        "    window = np.hanning(fft_length)[:, None]\n",
        "    window_norm = np.sum(window**2)\n",
        "\n",
        "    # The scaling below follows the convention of\n",
        "    # matplotlib.mlab.specgram which is the same as\n",
        "    # matlabs specgram.\n",
        "    scale = window_norm * sample_rate\n",
        "\n",
        "    trunc = (len(samples) - fft_length) % hop_length\n",
        "    x = samples[:len(samples) - trunc]\n",
        "\n",
        "    # \"stride trick\" reshape to include overlap\n",
        "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
        "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
        "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
        "\n",
        "    # window stride sanity check\n",
        "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
        "\n",
        "    # broadcast window, compute fft over columns and square mod\n",
        "    x = np.fft.rfft(x * window, axis=0)\n",
        "    x = np.absolute(x)**2\n",
        "\n",
        "    # scale, 2.0 for everything except dc and fft_length/2\n",
        "    x[1:-1, :] *= (2.0 / scale)\n",
        "    x[(0, -1), :] /= scale\n",
        "\n",
        "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
        "\n",
        "    return x, freqs"
      ],
      "metadata": {
        "id": "WMRrauu0i3pA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
        "                          eps=1e-14):\n",
        "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
        "    Params:\n",
        "        filename (str): Path to the audio file\n",
        "        step (int): Step size in milliseconds between windows\n",
        "        window (int): FFT window size in milliseconds\n",
        "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
        "            [0, max_freq] are returned\n",
        "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
        "    \"\"\"\n",
        "    with soundfile.SoundFile(filename) as sound_file:\n",
        "        audio = sound_file.read(dtype='float32')\n",
        "        sample_rate = sound_file.samplerate\n",
        "        if audio.ndim >= 2:\n",
        "            audio = np.mean(audio, 1)\n",
        "        if max_freq is None:\n",
        "            max_freq = sample_rate / 2\n",
        "        if max_freq > sample_rate / 2:\n",
        "            raise ValueError(\"max_freq must not be greater than half of \"\n",
        "                             \" sample rate\")\n",
        "        if step > window:\n",
        "            raise ValueError(\"step size must not be greater than window size\")\n",
        "        hop_length = int(0.001 * step * sample_rate)\n",
        "        fft_length = int(0.001 * window * sample_rate)\n",
        "        pxx, freqs = spectrogram(\n",
        "            audio, fft_length=fft_length, sample_rate=sample_rate,\n",
        "            hop_length=hop_length)\n",
        "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
        "    return np.transpose(np.log(pxx[:ind, :] + eps))"
      ],
      "metadata": {
        "id": "wOFao0Sii5cb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_int_sequence(text):\n",
        "    \"\"\" Convert text to an integer sequence \"\"\"\n",
        "    int_sequence = []\n",
        "    for c in text:\n",
        "        if c == ' ':\n",
        "            ch = char_map['<SPACE>']\n",
        "        else:\n",
        "            # print(\"checking character \" + c + \" in map:\")\n",
        "            # print(char_map)\n",
        "            ch = char_map[c]\n",
        "        int_sequence.append(ch)\n",
        "    return int_sequence"
      ],
      "metadata": {
        "id": "jCyHoVMdjhvz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def int_sequence_to_text(int_sequence):\n",
        "    \"\"\" Convert an integer sequence to text \"\"\"\n",
        "    text = []\n",
        "    for c in int_sequence:\n",
        "        ch = index_map[c]\n",
        "        text.append(ch)\n",
        "    return text"
      ],
      "metadata": {
        "id": "ubnnLGM_jn4u"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code adapted from https://martin-thoma.com/word-error-rate-calculation/\n",
        "def wer(r, h):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Levenshtein distance.\n",
        "\n",
        "    Works only for iterables up to 254 elements (uint8).\n",
        "    O(nm) time ans space complexity.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r : list\n",
        "    h : list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
        "    1\n",
        "    >>> wer(\"who is there\".split(), \"\".split())\n",
        "    3\n",
        "    >>> wer(\"\".split(), \"who is there\".split())\n",
        "    3\n",
        "    \"\"\"\n",
        "    # initialisation\n",
        "    import numpy\n",
        "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8)\n",
        "    d = d.reshape((len(r)+1, len(h)+1))\n",
        "    for i in range(len(r)+1):\n",
        "        for j in range(len(h)+1):\n",
        "            if i == 0:\n",
        "                d[0][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][0] = i\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                d[i][j] = d[i-1][j-1]\n",
        "            else:\n",
        "                substitution = d[i-1][j-1] + 1\n",
        "                insertion    = d[i][j-1] + 1\n",
        "                deletion     = d[i-1][j] + 1\n",
        "                d[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    return d[len(r)][len(h)]"
      ],
      "metadata": {
        "id": "EzhrIq8vjwgn"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Helper functions for plotting\n",
        "\"\"\"\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_hist(p):\n",
        "    hist = pickle.load(open( \"models/\" + p + \".pickle\", \"rb\"))\n",
        "    plt.plot(hist['loss'], label=\"train\")\n",
        "    plt.plot(hist['val_loss'], label=\"valid\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend()\n",
        "    plt.show"
      ],
      "metadata": {
        "id": "LpCfU5LTjyq_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input data generation"
      ],
      "metadata": {
        "id": "qVeZhHuEkJjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNG_SEED = 123\n",
        "\n",
        "def make_audio_gen(train_json,\n",
        "                   valid_json,\n",
        "                   minibatch_size=20,\n",
        "                   spectrogram=True,\n",
        "                   mfcc_dim=13,\n",
        "                   sort_by_duration=False,\n",
        "                   max_duration=10.0):\n",
        "    return AudioGenerator(train_json, valid_json, minibatch_size=minibatch_size, \n",
        "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
        "        sort_by_duration=sort_by_duration)\n"
      ],
      "metadata": {
        "id": "cl8Tojq3kLsf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Defines a class that is used to featurize audio clips, and provide\n",
        "them to the network for training or testing.\n",
        "\"\"\"\n",
        "class AudioGenerator():\n",
        "    def __init__(self, train_corpus, valid_corpus, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
        "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
        "        sort_by_duration=False):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
        "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
        "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
        "                [0, max_freq] are returned (for spectrogram ONLY)\n",
        "            desc_file (str, optional): Path to a JSON-line file that contains\n",
        "                labels and paths to the audio files. If this is None, then\n",
        "                load metadata right away\n",
        "        \"\"\"\n",
        "        self.train_corpus = train_corpus\n",
        "        self.valid_corpus = valid_corpus\n",
        "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
        "        self.mfcc_dim = mfcc_dim\n",
        "        self.feats_mean = np.zeros((self.feat_dim,))\n",
        "        self.feats_std = np.ones((self.feat_dim,))\n",
        "        self.rng = random.Random(RNG_SEED)\n",
        "        if desc_file is not None:\n",
        "            self.load_metadata_from_desc_file(desc_file)\n",
        "        self.step = step\n",
        "        self.window = window\n",
        "        self.max_freq = max_freq\n",
        "        self.cur_train_index = 0\n",
        "        self.cur_valid_index = 0\n",
        "        self.cur_test_index = 0\n",
        "        self.max_duration=max_duration\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.spectrogram = spectrogram\n",
        "        self.sort_by_duration = sort_by_duration\n",
        "\n",
        "    def get_batch(self, partition):\n",
        "        \"\"\" Obtain a batch of train, validation, or test data\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            audio_paths = self.train_audio_paths\n",
        "            cur_index = self.cur_train_index\n",
        "            texts = self.train_texts\n",
        "        elif partition == 'valid':\n",
        "            audio_paths = self.valid_audio_paths\n",
        "            cur_index = self.cur_valid_index\n",
        "            texts = self.valid_texts\n",
        "        elif partition == 'test':\n",
        "            audio_paths = self.test_audio_paths\n",
        "            cur_index = self.test_valid_index\n",
        "            texts = self.test_texts\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "        features = [self.normalize(self.featurize(a)) for a in \n",
        "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
        "\n",
        "        # calculate necessary sizes\n",
        "        max_length = max([features[i].shape[0] \n",
        "            for i in range(0, self.minibatch_size)])\n",
        "        max_string_length = max([len(texts[cur_index+i]) \n",
        "            for i in range(0, self.minibatch_size)])\n",
        "        \n",
        "        # initialize the arrays\n",
        "        X_data = np.zeros([self.minibatch_size, max_length, \n",
        "            self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
        "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
        "        input_length = np.zeros([self.minibatch_size, 1])\n",
        "        label_length = np.zeros([self.minibatch_size, 1])\n",
        "        \n",
        "        for i in range(0, self.minibatch_size):\n",
        "            # calculate X_data & input_length\n",
        "            feat = features[i]\n",
        "            input_length[i] = feat.shape[0]\n",
        "            X_data[i, :feat.shape[0], :] = feat\n",
        "\n",
        "            # calculate labels & label_length\n",
        "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
        "            labels[i, :len(label)] = label\n",
        "            label_length[i] = len(label)\n",
        " \n",
        "        # return the arrays\n",
        "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
        "        inputs = {'the_input': X_data, \n",
        "                  'the_labels': labels, \n",
        "                  'input_length': input_length, \n",
        "                  'label_length': label_length \n",
        "                 }\n",
        "        return (inputs, outputs)\n",
        "\n",
        "    def shuffle_data_by_partition(self, partition):\n",
        "        \"\"\" Shuffle the training or validation data\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_data(\n",
        "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
        "            self.train_length = len(self.train_texts)\n",
        "        elif partition == 'valid':\n",
        "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
        "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
        "            self.valid_length = len(self.valid_texts)\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "    def sort_data_by_duration(self, partition):\n",
        "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
        "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
        "        elif partition == 'valid':\n",
        "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
        "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "    def next_train(self):\n",
        "        \"\"\" Obtain a batch of training data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('train')\n",
        "            self.cur_train_index += self.minibatch_size\n",
        "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
        "                self.cur_train_index = 0\n",
        "                self.shuffle_data_by_partition('train')\n",
        "            yield ret    \n",
        "\n",
        "    def next_valid(self):\n",
        "        \"\"\" Obtain a batch of validation data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('valid')\n",
        "            self.cur_valid_index += self.minibatch_size\n",
        "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
        "                self.cur_valid_index = 0\n",
        "                self.shuffle_data_by_partition('valid')\n",
        "            yield ret\n",
        "\n",
        "    def next_test(self):\n",
        "        \"\"\" Obtain a batch of test data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('test')\n",
        "            self.cur_test_index += self.minibatch_size\n",
        "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
        "                self.cur_test_index = 0\n",
        "            yield ret\n",
        "\n",
        "    def load_train_data(self):\n",
        "        desc_file=self.train_corpus\n",
        "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
        "        self.fit_train()\n",
        "        if self.sort_by_duration:\n",
        "            self.sort_data_by_duration('train')\n",
        "\n",
        "    def load_validation_data(self):\n",
        "        desc_file=self.valid_corpus\n",
        "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
        "        if self.sort_by_duration:\n",
        "            self.sort_data_by_duration('valid')\n",
        "\n",
        "    def load_test_data(self):\n",
        "        desc_file='test_corpus.json'\n",
        "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
        "    \n",
        "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
        "        \"\"\" Read metadata from a JSON-line file\n",
        "            (possibly takes long, depending on the filesize)\n",
        "        Params:\n",
        "            desc_file (str):  Path to a JSON-line file that contains labels and\n",
        "                paths to the audio files\n",
        "            partition (str): One of 'train', 'validation' or 'test'\n",
        "        \"\"\"\n",
        "        audio_paths, durations, texts = [], [], []\n",
        "        with open(desc_file, encoding='utf-8') as json_line_file:\n",
        "            for line_num, json_line in enumerate(json_line_file):\n",
        "                try:\n",
        "                    spec = json.loads(json_line)\n",
        "                    if float(spec['duration']) > self.max_duration:\n",
        "                        continue\n",
        "                    audio_paths.append(spec['key'])\n",
        "                    durations.append(float(spec['duration']))\n",
        "                    texts.append(spec['text'])\n",
        "                except Exception as e:\n",
        "                    # Change to (KeyError, ValueError) or\n",
        "                    # (KeyError,json.decoder.JSONDecodeError), depending on\n",
        "                    # json module version\n",
        "                    print('Error reading line #{}: {}'\n",
        "                                .format(line_num, json_line))\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths = audio_paths\n",
        "            self.train_durations = durations\n",
        "            self.train_texts = texts\n",
        "        elif partition == 'validation':\n",
        "            self.valid_audio_paths = audio_paths\n",
        "            self.valid_durations = durations\n",
        "            self.valid_texts = texts\n",
        "        elif partition == 'test':\n",
        "            self.test_audio_paths = audio_paths\n",
        "            self.test_durations = durations\n",
        "            self.test_texts = texts\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition to load metadata. \"\n",
        "             \"Must be train/validation/test\")\n",
        "            \n",
        "    def fit_train(self, k_samples=100):\n",
        "        \"\"\" Estimate the mean and std of the features from the training set\n",
        "        Params:\n",
        "            k_samples (int): Use this number of samples for estimation\n",
        "        \"\"\"\n",
        "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
        "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
        "        feats = [self.featurize(s) for s in samples]\n",
        "        feats = np.vstack(feats)\n",
        "        self.feats_mean = np.mean(feats, axis=0)\n",
        "        self.feats_std = np.std(feats, axis=0)\n",
        "        \n",
        "    def featurize(self, audio_clip):\n",
        "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
        "        Params:\n",
        "            audio_clip (str): Path to the audio clip\n",
        "        \"\"\"\n",
        "        if self.spectrogram:\n",
        "            return spectrogram_from_file(\n",
        "                audio_clip, step=self.step, window=self.window,\n",
        "                max_freq=self.max_freq)\n",
        "        else:\n",
        "            (rate, sig) = wav.read(audio_clip)\n",
        "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
        "\n",
        "    def normalize(self, feature, eps=1e-14):\n",
        "        \"\"\" Center a feature using the mean and std\n",
        "        Params:\n",
        "            feature (numpy.ndarray): Feature to normalize\n",
        "        \"\"\"\n",
        "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
        "\n",
        "    def train_length(self):\n",
        "        return len(self.train_texts)\n",
        "    \n",
        "    def valid_length(self):\n",
        "        return len(self.valid_texts)\n",
        "\n",
        "\n",
        "def shuffle_data(audio_paths, durations, texts):\n",
        "    \"\"\" Shuffle the data (called after making a complete pass through \n",
        "        training or validation data during the training process)\n",
        "    Params:\n",
        "        audio_paths (list): Paths to audio clips\n",
        "        durations (list): Durations of utterances for each audio clip\n",
        "        texts (list): Sentences uttered in each audio clip\n",
        "    \"\"\"\n",
        "    p = np.random.permutation(len(audio_paths))\n",
        "    audio_paths = [audio_paths[i] for i in p] \n",
        "    durations = [durations[i] for i in p] \n",
        "    texts = [texts[i] for i in p]\n",
        "    return audio_paths, durations, texts\n",
        "\n",
        "def sort_data(audio_paths, durations, texts):\n",
        "    \"\"\" Sort the data by duration \n",
        "    Params:\n",
        "        audio_paths (list): Paths to audio clips\n",
        "        durations (list): Durations of utterances for each audio clip\n",
        "        texts (list): Sentences uttered in each audio clip\n",
        "    \"\"\"\n",
        "    p = np.argsort(durations).tolist()\n",
        "    audio_paths = [audio_paths[i] for i in p]\n",
        "    durations = [durations[i] for i in p] \n",
        "    texts = [texts[i] for i in p]\n",
        "    return audio_paths, durations, texts\n",
        "\n",
        "def vis_train_features(index=0):\n",
        "    \"\"\" Visualizing the data point in the training set at the supplied index\n",
        "    \"\"\"\n",
        "    # obtain spectrogram\n",
        "    audio_gen = AudioGenerator(spectrogram=True)\n",
        "    audio_gen.load_train_data()\n",
        "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
        "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
        "    # obtain mfcc\n",
        "    audio_gen = AudioGenerator(spectrogram=False)\n",
        "    audio_gen.load_train_data()\n",
        "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
        "    # obtain text label\n",
        "    vis_text = audio_gen.train_texts[index]\n",
        "    # obtain raw audio\n",
        "    vis_raw_audio, _ = librosa.load(amharic_path(vis_audio_path))\n",
        "    # print total number of training examples\n",
        "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
        "    # return labels for plotting\n",
        "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
        "\n",
        "\n",
        "def plot_raw_audio(vis_raw_audio, title='Audio Signal', size=(12, 3)):\n",
        "    fig = plt.figure(figsize=size)\n",
        "    ax = fig.add_subplot(111)\n",
        "    steps = len(vis_raw_audio)\n",
        "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "\n",
        "def plot_mfcc_feature(vis_mfcc_feature):\n",
        "    # plot the MFCC feature\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
        "    plt.title('Normalized MFCC')\n",
        "    plt.ylabel('Time')\n",
        "    plt.xlabel('MFCC Coefficient')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
        "    plt.show()\n",
        "\n",
        "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
        "    # plot the normalized spectrogram\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
        "    plt.title('Normalized Spectrogram')\n",
        "    plt.ylabel('Time')\n",
        "    plt.xlabel('Frequency')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DvhOo8cWkRkX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model defination"
      ],
      "metadata": {
        "id": "FHeDhrOzkmdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_1(input_dim, units, activation, output_dim=29):\n",
        "    \"\"\" Build a recurrent network for speech \n",
        "    \"\"\"\n",
        "    # Main acoustic input\n",
        "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
        "    # Add recurrent layer\n",
        "    simp_rnn = GRU(units, activation=activation,\n",
        "        return_sequences=True, implementation=2, name='rnn')(input_data)\n",
        "    # TODO: Add batch normalization \n",
        "    bn_rnn = BatchNormalization()(simp_rnn)\n",
        "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
        "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
        "    # Add softmax activation layer\n",
        "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
        "    # Specify the model\n",
        "    model = Model(inputs=input_data, outputs=y_pred)\n",
        "    model.output_length = lambda x: x\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='models/model_1.png')\n",
        "    return model"
      ],
      "metadata": {
        "id": "JzMiRvY8knmT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training "
      ],
      "metadata": {
        "id": "wP3D5TJbkuR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "metadata": {
        "id": "Wa9EBFS6kr05"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_ctc_loss(input_to_softmax):\n",
        "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
        "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
        "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
        "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
        "    # CTC loss is implemented in a lambda layer\n",
        "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
        "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
        "    model = Model(\n",
        "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
        "        outputs=loss_out)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JJG4Dz5Nk7T8"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(audio_gen,\n",
        "          input_to_softmax, \n",
        "          model_name,\n",
        "          minibatch_size=20,\n",
        "          optimizer=SGD(learning_rate=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
        "          epochs=20,\n",
        "          verbose=1):    \n",
        "    # calculate steps_per_epoch\n",
        "    num_train_examples=len(audio_gen.train_audio_paths)\n",
        "    steps_per_epoch = num_train_examples//minibatch_size\n",
        "    # calculate validation_steps\n",
        "    num_valid_samples = len(audio_gen.valid_audio_paths) \n",
        "    validation_steps = num_valid_samples//minibatch_size\n",
        "    \n",
        "    # add CTC loss to the NN specified in input_to_softmax\n",
        "    model = add_ctc_loss(input_to_softmax)\n",
        "\n",
        "    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
        "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
        "\n",
        "    # make results/ directory, if necessary\n",
        "    if not os.path.exists('models'):\n",
        "        os.makedirs('models')\n",
        "\n",
        "    # add checkpointer\n",
        "    checkpointer = ModelCheckpoint(filepath='models/'+model_name+'.h5', verbose=0)\n",
        "\n",
        "    # train the model\n",
        "    hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
        "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps,\n",
        "        callbacks=[checkpointer], verbose=verbose, use_multiprocessing=True)\n",
        "\n",
        "    # save model loss\n",
        "    with open('models/'+model_name+'.pickle', 'wb') as f:\n",
        "        pickle.dump(hist.history, f)"
      ],
      "metadata": {
        "id": "ep-JpKCtk_yI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training RNN model"
      ],
      "metadata": {
        "id": "ro2L6LGelI29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OhnTCrGclFaN"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}